{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d9863e-5efe-45bf-abc6-55b8b3388e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\janan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b893c6-7490-4ed7-bd83-bf0dbc316c1a",
   "metadata": {},
   "source": [
    "                                                  Data Scraping Overview\n",
    "                                                  \n",
    "This project involves scraping restaurant data from Deliveroo using Selenium and BeautifulSoup. The goal is to collect data on various restaurants in Nottingham, specifically in the Sneinton Bakersfield area, filtered by different cuisines.\n",
    "\n",
    "Tools and Libraries Used:\n",
    "\n",
    "1. Pandas: For data manipulation and creating a DataFrame to store the restaurant data.\n",
    "2. BeautifulSoup: A Python library for parsing HTML and XML documents. It is used here to extract relevant information from the webpage.\n",
    "3. Selenium: A web automation tool that allows for dynamic interaction with web pages. It is used for opening the webpage, handling cookies, and scrolling to load all restaurant listings.\n",
    "4. Regular Expressions (re): Employed for searching and extracting specific patterns from the HTML content, such as ratings and delivery costs.\n",
    "\n",
    "\n",
    "STEP 1: Setup:\n",
    "\n",
    "* Defined the target postcode and a list of cuisines to filter the restaurants.\n",
    "* Initialized the Selenium WebDriver to control the Chrome browser.\n",
    "\n",
    "STEP 2: Scraping Logic:\n",
    "\n",
    "* Iterated through the predefined list of cuisines, constructing the appropriate URL for each cuisine on Deliveroo.\n",
    "* Opened the webpage for each cuisine and accepted cookies if prompted.\n",
    "* Implemented a scrolling mechanism to ensure all restaurant listings were loaded dynamically.\n",
    "\n",
    "STEP 3: Data Extraction:\n",
    "\n",
    "Utilized BeautifulSoup to parse the loaded HTML content and extract relevant data such as:\n",
    "* Restaurant name\n",
    "* Distance to the delivery location\n",
    "* Delivery cost\n",
    "* Restaurant rating\n",
    "* Hyperlink to the restaurant's menu\n",
    "Each piece of information was gathered using specific functions designed to locate and retrieve the required data.\n",
    "\n",
    "STEP 4: Data Storage:\n",
    "\n",
    "* Compiled all extracted data into a list of dictionaries, with each dictionary representing a restaurant.\n",
    "* Converted the list into a Pandas DataFrame for easy analysis and manipulation.\n",
    "  \n",
    "STEP 5: Output:\n",
    "\n",
    "The final DataFrame, restaurant_df1, contains comprehensive information about each restaurant, including its rank, name, distance, delivery cost, rating, and cuisine type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235a250-23c8-45b5-9a5d-59fe6c4da9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Postcode and cuisine list\n",
    "postcode = 'NG2 4AP'\n",
    "cuisines = [\n",
    "    'all', 'day', 'breakfast', 'american', 'asian', 'breakfast', \n",
    "    'british', 'brunch', 'café', 'Caribbean', 'Chinese', \n",
    "    'drinks', 'grocery', 'healthy', 'indian', 'italian', \n",
    "    'jamaican', 'japanese', 'mexican', 'shopping', 'thai', \n",
    "    'turkish'\n",
    "]\n",
    "\n",
    "chrome_driver_path = r\"C:\\Users\\janan\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "initial_load_time = 2\n",
    "scroll_pause_time = 0.0001\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Create an empty list to store all restaurant data\n",
    "all_restaurants_list = []\n",
    "\n",
    "# Loop through each cuisine\n",
    "for cuisine in cuisines:\n",
    "    url = f'https://deliveroo.co.uk/restaurants/nottingham/sneinton-bakersfield?postcode={postcode}&cuisine={cuisine}&collection=all-restaurants'\n",
    "    \n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "    time.sleep(initial_load_time)\n",
    "\n",
    "    # Get screen height for scrolling\n",
    "    screen_height = driver.execute_script('return window.screen.height;')\n",
    "\n",
    "    # Accept cookies\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        cookie_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[text()=\"Accept All\"]')))\n",
    "        driver.execute_script(\"arguments[0].click();\", cookie_button)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not find or click the accept cookies button: {e}\")\n",
    "\n",
    "    # Scroll down the page\n",
    "    i = 1\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo(0, {screen_height} * {i});\")\n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if screen_height * i > scroll_height:\n",
    "            print(\"Reached the bottom of the page.\")\n",
    "            break\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Store class names\n",
    "    a_tag_class = 'HomeFeedUICard-3e299003014c14f9'\n",
    "    links_html = [str(x) for x in soup.find_all(class_=a_tag_class)]\n",
    "    \n",
    "    a_tag_class_restaurant = 'HomeFeedUICard-619f89858093a1f4'\n",
    "    restaurants_html = [str(x) for x in soup.find_all(class_=a_tag_class_restaurant)]\n",
    "\n",
    "    # Define data extraction functions\n",
    "    def find_rating(links):\n",
    "        link_tag = links.find('a')\n",
    "        if link_tag and 'aria-label' in link_tag.attrs:\n",
    "            aria_label = link_tag['aria-label']\n",
    "            rating_match = re.search(r'Rated\\s+([\\d.]+)', aria_label)\n",
    "            if rating_match:\n",
    "                return rating_match.group(1)\n",
    "        return \"No rating found\"\n",
    "\n",
    "    def restaurant_name(restaurant):\n",
    "        return restaurant.find('p').text.strip()\n",
    "\n",
    "    def find_distance_data(restaurant):\n",
    "        name_tag = restaurant.find('span')\n",
    "        name = name_tag.text.strip()\n",
    "        distance_match = re.search(r'(\\d+\\.?\\d*)\\s*mi', name)\n",
    "        return distance_match.group(0) if distance_match else \"No distance found\"\n",
    "\n",
    "    def find_delivery_cost(restaurant):\n",
    "        name_tag = restaurant.find('span')\n",
    "        name = name_tag.text.strip()\n",
    "        match = re.search(r'£(\\d+\\.?\\d*)\\s*delivery', name)\n",
    "        return match.group(0) if match else \"Free delivery\"\n",
    "\n",
    "    def find_hyperlink(restaurant):\n",
    "        link_tag = restaurant.find('a')\n",
    "        if link_tag and 'href' in link_tag.attrs:\n",
    "            return 'http://deliveroo.co.uk' + link_tag['href']\n",
    "        return \"No link found\"\n",
    "\n",
    "    # Extract restaurant data for the current cuisine\n",
    "    for restaurant_html, links_html in zip(restaurants_html, links_html):\n",
    "        restaurant = BeautifulSoup(restaurant_html, 'html.parser')\n",
    "        links = BeautifulSoup(links_html, 'html.parser')\n",
    "\n",
    "        restaurant_dictionary = {\n",
    "            'rank': len(all_restaurants_list) + 1,\n",
    "            'restaurant_name': restaurant_name(restaurant),\n",
    "            'distance': find_distance_data(restaurant),\n",
    "            'delivery_cost': find_delivery_cost(restaurant),\n",
    "            'link': find_hyperlink(links),\n",
    "            'rating': find_rating(links),\n",
    "            'cuisine': cuisine  # Add the current cuisine\n",
    "        }\n",
    "        \n",
    "        all_restaurants_list.append(restaurant_dictionary)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the aggregated data\n",
    "restaurant_df1 = pd.DataFrame(all_restaurants_list)\n",
    "restaurant_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ad96dd0-aa30-444d-838a-c17e7c3ea877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to restaurants1_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the aggregated data\n",
    "restaurant_df1 = pd.DataFrame(all_restaurants_list)\n",
    "\n",
    "# Specify the filename\n",
    "csv_filename = 'restaurants1_data.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "restaurant_df1.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd579bb-9667-48ad-b4b5-6f6f64f2ced3",
   "metadata": {},
   "source": [
    "                                                    2. WEB SCRAPING BASED ON DISHES SERVED\n",
    "\n",
    "1. Navigating the Web Page: For each dish, we construct a URL to access the corresponding restaurant listings on Deliveroo. The script opens the URL and waits for the page to load.\n",
    "\n",
    "2. Handling Cookies: Since many websites require users to accept cookies, the script looks for the cookie consent button and clicks it if found.\n",
    "\n",
    "3. Scrolling for Data: To ensure we capture all restaurant listings on the page, the script scrolls down repeatedly until it reaches the bottom of the page. This is necessary because the website loads additional content dynamically as you scroll.\n",
    "\n",
    "4. Data Extraction: Using BeautifulSoup, we parse the page's HTML to find the relevant restaurant information. We define several functions to extract:\n",
    "\n",
    "* Restaurant Name: The name of the restaurant.\n",
    "* Rating: The rating of the restaurant, extracted from the aria-label attribute of links.\n",
    "* Distance: The distance from the postcode, parsed from a specific span tag.\n",
    "* Delivery Cost: The cost of delivery, also extracted from a span tag.\n",
    "* Link: The URL linking to the restaurant's page on Deliveroo.\n",
    "* Storing Data: For each restaurant, we create a dictionary containing all extracted information and append it to a list. After processing all dishes, we close the WebDriver.\n",
    "\n",
    "5. Creating a DataFrame: Finally, we convert the list of dictionaries into a Pandas DataFrame, providing a structured format for further analysis.\n",
    "\n",
    "Output:\n",
    "\n",
    "The output of this project is a DataFrame containing detailed information about various restaurants, including their names, ratings, delivery costs, distances, and associated dish types. This data can be used for further analysis, such as comparing ratings against delivery costs or visualizing the distribution of distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9f047-ff85-4410-8a5e-4c1ada8550dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Postcode and cuisine list\n",
    "postcode = 'NG2 4AP'\n",
    "dish = [\n",
    "    'alcohol', 'burgers', 'cakes', 'chicken', 'coffee', 'curry', \n",
    "    'dessert', 'fish+and+chips', 'fried+chicken', 'fries', \n",
    "    'kebab', 'milkshakes', 'noodles', 'pizza', 'salads', \n",
    "    'sandwiches', 'seafood', 'wraps'\n",
    "]\n",
    "chrome_driver_path = r\"C:\\Users\\janan\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "initial_load_time = 2\n",
    "scroll_pause_time = 0.0001\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Create an empty list to store all restaurant data\n",
    "all_restaurants_list = []\n",
    "\n",
    "# Loop through each cuisine\n",
    "for dish in dish:\n",
    "    url = f'https://deliveroo.co.uk/restaurants/nottingham/sneinton-bakersfield?postcode={postcode}&dish={dish}&collection=all-restaurants'\n",
    "    \n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "    time.sleep(initial_load_time)\n",
    "\n",
    "    # Get screen height for scrolling\n",
    "    screen_height = driver.execute_script('return window.screen.height;')\n",
    "\n",
    "    # Accept cookies\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        cookie_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[text()=\"Accept All\"]')))\n",
    "        driver.execute_script(\"arguments[0].click();\", cookie_button)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not find or click the accept cookies button: {e}\")\n",
    "\n",
    "    # Scroll down the page\n",
    "    i = 1\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo(0, {screen_height} * {i});\")\n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if screen_height * i > scroll_height:\n",
    "            print(\"Reached the bottom of the page.\")\n",
    "            break\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Store class names\n",
    "    a_tag_class = 'HomeFeedUICard-3e299003014c14f9'\n",
    "    links_html = [str(x) for x in soup.find_all(class_=a_tag_class)]\n",
    "    \n",
    "    a_tag_class_restaurant = 'HomeFeedUICard-619f89858093a1f4'\n",
    "    restaurants_html = [str(x) for x in soup.find_all(class_=a_tag_class_restaurant)]\n",
    "\n",
    "    # Define data extraction functions\n",
    "    def find_rating(links):\n",
    "        link_tag = links.find('a')\n",
    "        if link_tag and 'aria-label' in link_tag.attrs:\n",
    "            aria_label = link_tag['aria-label']\n",
    "            rating_match = re.search(r'Rated\\s+([\\d.]+)', aria_label)\n",
    "            if rating_match:\n",
    "                return rating_match.group(1)\n",
    "        return \"No rating found\"\n",
    "\n",
    "    def restaurant_name(restaurant):\n",
    "        return restaurant.find('p').text.strip()\n",
    "\n",
    "    def find_distance_data(restaurant):\n",
    "        name_tag = restaurant.find('span')\n",
    "        name = name_tag.text.strip()\n",
    "        distance_match = re.search(r'(\\d+\\.?\\d*)\\s*mi', name)\n",
    "        return distance_match.group(0) if distance_match else \"No distance found\"\n",
    "\n",
    "    def find_delivery_cost(restaurant):\n",
    "        name_tag = restaurant.find('span')\n",
    "        name = name_tag.text.strip()\n",
    "        match = re.search(r'£(\\d+\\.?\\d*)\\s*delivery', name)\n",
    "        return match.group(0) if match else \"Free delivery\"\n",
    "\n",
    "    def find_hyperlink(restaurant):\n",
    "        link_tag = restaurant.find('a')\n",
    "        if link_tag and 'href' in link_tag.attrs:\n",
    "            return 'http://deliveroo.co.uk' + link_tag['href']\n",
    "        return \"No link found\"\n",
    "\n",
    "    # Extract restaurant data for the current cuisine\n",
    "    for restaurant_html, links_html in zip(restaurants_html, links_html):\n",
    "        restaurant = BeautifulSoup(restaurant_html, 'html.parser')\n",
    "        links = BeautifulSoup(links_html, 'html.parser')\n",
    "\n",
    "        restaurant_dictionary = {\n",
    "            'rank': len(all_restaurants_list) + 1,\n",
    "            'restaurant_name': restaurant_name(restaurant),\n",
    "            'distance': find_distance_data(restaurant),\n",
    "            'delivery_cost': find_delivery_cost(restaurant),\n",
    "            'link': find_hyperlink(links),\n",
    "            'rating': find_rating(links),\n",
    "            'dish': dish  # Add the current cuisine\n",
    "        }\n",
    "        \n",
    "        all_restaurants_list.append(restaurant_dictionary)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the aggregated data\n",
    "restaurant_df = pd.DataFrame(all_restaurants_list)\n",
    "restaurant_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507a055e-abca-4412-9de1-e525054ac359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to dish_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the aggregated data\n",
    "restaurant_df = pd.DataFrame(all_restaurants_list)\n",
    "\n",
    "# Specify the filename\n",
    "csv_filename = 'dish_data.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "restaurant_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab279625-2862-465c-8e3c-bea542927f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
